{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "###### PART A: Transformer architecture and training ############"
      ],
      "metadata": {
        "id": "weqSeaooj47q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xEKoPe2y6-V",
        "outputId": "39a7ec4e-4ed6-4ad4-829b-f554b455417e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3321/3321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m761s\u001b[0m 225ms/step - accuracy: 0.2335 - loss: 1.8251 - val_accuracy: 0.3212 - val_loss: 1.6799\n"
          ]
        }
      ],
      "source": [
        "## Working Tx code\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, d_model=64, num_heads=4, ff_dim=128, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='gelu'),\n",
        "            layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout)\n",
        "        self.dropout2 = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.attn(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"d_model\": 64,\n",
        "            \"num_heads\": 4,\n",
        "            \"ff_dim\": 128,\n",
        "            \"dropout\": 0.1\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class Connect4Transformer(Model):\n",
        "    def __init__(self, d_model=64, num_heads=4, ff_dim=128, num_layers=3, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)  # Pass **kwargs to the parent class\n",
        "        self.d_model = d_model\n",
        "        self.input_projection = layers.Dense(d_model)\n",
        "        self.row_embed = layers.Embedding(input_dim=6, output_dim=d_model // 2)\n",
        "        self.col_embed = layers.Embedding(input_dim=7, output_dim=d_model // 2)\n",
        "        self.transformer_blocks = [TransformerBlock(d_model, num_heads, ff_dim, dropout) for _ in range(num_layers)]\n",
        "        self.column_attention = layers.MultiHeadAttention(num_heads=2, key_dim=d_model)\n",
        "        self.output_layer = layers.Dense(7, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.reshape(inputs, (-1, 6, 7, 2))\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        rows = tf.tile(tf.range(6)[None, :, None], [batch_size, 1, 7])\n",
        "        cols = tf.tile(tf.range(7)[None, None, :], [batch_size, 6, 1])\n",
        "\n",
        "        row_emb = self.row_embed(rows)\n",
        "        col_emb = self.col_embed(cols)\n",
        "        pos_encoding = tf.concat([row_emb, col_emb], axis=-1)\n",
        "\n",
        "        x = self.input_projection(x)\n",
        "        x += pos_encoding\n",
        "\n",
        "        x = tf.reshape(x, (-1, 6 * 7, self.d_model))\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x)\n",
        "\n",
        "        column_queries = tf.tile(tf.range(7)[None, :, None], [batch_size, 1, self.d_model])\n",
        "        context = self.column_attention(column_queries, x)\n",
        "\n",
        "        return self.output_layer(context[:, :, 0])\n",
        "\n",
        "\n",
        "def load_and_preprocess(data):\n",
        "    X = data.iloc[:, :-1].values.astype(np.float32)\n",
        "    y = data.iloc[:, -1].values.astype(np.int32)\n",
        "    X = X.reshape(-1, 6, 7, 2)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "def create_model():\n",
        "    model = Connect4Transformer(\n",
        "        d_model=64,\n",
        "        num_heads=8,\n",
        "        ff_dim=128,\n",
        "        num_layers=3,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(3e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv(\"converted_board_with_play_y.csv\")\n",
        "    X_train, X_val, y_train, y_val = load_and_preprocess(df)\n",
        "    model = create_model()\n",
        "\n",
        "    # Build the model by calling it once\n",
        "    dummy_input = tf.zeros((1, 6, 7, 2))\n",
        "    _ = model(dummy_input)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=64,\n",
        "        epochs=10\n",
        "    )\n",
        "\n",
        "    # model.save('tx.h5')\n",
        "    model.save('Converted_8H_10E_64B.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### PART B: Retrieve the model ############"
      ],
      "metadata": {
        "id": "GUwMRcQ44ASn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define the custom TransformerBlock and Connect4Transformer classes\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model=64, num_heads=4, ff_dim=128, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='gelu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.attn(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"d_model\": 64,\n",
        "            \"num_heads\": 8,\n",
        "            \"ff_dim\": 128,\n",
        "            \"dropout\": 0.1\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "class Connect4Transformer(tf.keras.Model):\n",
        "    def __init__(self, d_model=64, num_heads=8, ff_dim=128, num_layers=3, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.input_projection = tf.keras.layers.Dense(d_model)\n",
        "        self.row_embed = tf.keras.layers.Embedding(input_dim=6, output_dim=d_model // 2)\n",
        "        self.col_embed = tf.keras.layers.Embedding(input_dim=7, output_dim=d_model // 2)\n",
        "        self.transformer_blocks = [TransformerBlock(d_model, num_heads, ff_dim, dropout) for _ in range(num_layers)]\n",
        "        self.column_attention = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=d_model)\n",
        "        self.output_layer = tf.keras.layers.Dense(7, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.reshape(inputs, (-1, 6, 7, 2))\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        rows = tf.tile(tf.range(6)[None, :, None], [batch_size, 1, 7])\n",
        "        cols = tf.tile(tf.range(7)[None, None, :], [batch_size, 6, 1])\n",
        "\n",
        "        row_emb = self.row_embed(rows)\n",
        "        col_emb = self.col_embed(cols)\n",
        "        pos_encoding = tf.concat([row_emb, col_emb], axis=-1)\n",
        "\n",
        "        x = self.input_projection(x)\n",
        "        x += pos_encoding\n",
        "\n",
        "        x = tf.reshape(x, (-1, 6 * 7, self.d_model))\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x)\n",
        "\n",
        "        column_queries = tf.tile(tf.range(7)[None, :, None], [batch_size, 1, self.d_model])\n",
        "        context = self.column_attention(column_queries, x)\n",
        "\n",
        "        return self.output_layer(context[:, :, 0])\n",
        "\n",
        "# Load the model with custom objects\n",
        "model = load_model('Converted_8H_10E_64B.keras', custom_objects={'TransformerBlock': TransformerBlock, 'Connect4Transformer': Connect4Transformer})\n",
        "\n",
        "# Print model summary to verify\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "TcaDXSLgzgIP",
        "outputId": "212012b6-1eb3-476d-b492-f6b4741fbc76"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"connect4_transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"connect4_transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_56 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │             \u001b[38;5;34m192\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_14 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │             \u001b[38;5;34m192\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_15 (\u001b[38;5;33mEmbedding\u001b[0m)             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m32\u001b[0m)               │             \u001b[38;5;34m224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_21                 │ ?                           │          \u001b[38;5;34m33,472\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_22                 │ ?                           │          \u001b[38;5;34m33,472\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_23                 │ ?                           │          \u001b[38;5;34m33,472\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ multi_head_attention_31              │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m33,216\u001b[0m │\n",
              "│ (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_63 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m7\u001b[0m)                      │              \u001b[38;5;34m56\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_21                 │ ?                           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_22                 │ ?                           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block_23                 │ ?                           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ multi_head_attention_31              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,216</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m402,890\u001b[0m (1.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">402,890</span> (1.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m134,296\u001b[0m (524.59 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">134,296</span> (524.59 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m268,594\u001b[0m (1.02 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">268,594</span> (1.02 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPKbSKa0382E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-45L6Hqi5gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMpHHs4_i5ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###### PART C: Play the game ############"
      ],
      "metadata": {
        "id": "5acTJU8si5lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: download model to local\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"connect4_final_1\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1CCgmf1e6eGHhf6_3p7b5wg8If0scxkWo\n",
        "\"\"\"\n",
        "\n",
        "#import anvil.server\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ[\"TF_TRT_ALLOW_BUILD_FAILURE\"] = \"1\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# Replace with your actual Anvil Uplink Key\n",
        "#ANVIL_UPLINK_KEY = \"server_YRMKNBRJCQVXQ7JPEMRYXMUL-BLGMXSSKTGUJGMP2\"\n",
        "\n",
        "# Connect to Anvil\n",
        "#anvil.server.connect(ANVIL_UPLINK_KEY)\n",
        "\n",
        "print(\"Connected to Anvil successfully!\")\n",
        "\n",
        "#@anvil.server.callable\n",
        "def initialize_board():\n",
        "    \"\"\"Initialize an empty Connect 4 board.\"\"\"\n",
        "    return np.zeros((6, 7, 2), dtype=int).tolist()  # Convert to list for client-side compatibility\n",
        "\n",
        "#@anvil.server.callable\n",
        "def drop_token(board, column, player):\n",
        "    \"\"\"Drop a token into the specified column.\"\"\"\n",
        "    board = np.array(board)  # Convert list back to NumPy array\n",
        "\n",
        "    for row in range(5, -1, -1):\n",
        "        if board[row, column, 0] == 0:\n",
        "            board[row, column, 0] = player\n",
        "            return board.tolist()  # Convert back to list before returning\n",
        "\n",
        "    return board.tolist()  # Return unchanged board if column is full\n",
        "\n",
        "###################\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Define the custom TransformerBlock and Connect4Transformer classes\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model=64, num_heads=4, ff_dim=128, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attn = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation='gelu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.attn(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"d_model\": 64,\n",
        "            \"num_heads\": 8,\n",
        "            \"ff_dim\": 128,\n",
        "            \"dropout\": 0.1\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "class Connect4Transformer(tf.keras.Model):\n",
        "    def __init__(self, d_model=64, num_heads=8, ff_dim=128, num_layers=3, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.input_projection = tf.keras.layers.Dense(d_model)\n",
        "        self.row_embed = tf.keras.layers.Embedding(input_dim=6, output_dim=d_model // 2)\n",
        "        self.col_embed = tf.keras.layers.Embedding(input_dim=7, output_dim=d_model // 2)\n",
        "        self.transformer_blocks = [TransformerBlock(d_model, num_heads, ff_dim, dropout) for _ in range(num_layers)]\n",
        "        self.column_attention = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=d_model)\n",
        "        self.output_layer = tf.keras.layers.Dense(7, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.reshape(inputs, (-1, 6, 7, 2))\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        rows = tf.tile(tf.range(6)[None, :, None], [batch_size, 1, 7])\n",
        "        cols = tf.tile(tf.range(7)[None, None, :], [batch_size, 6, 1])\n",
        "\n",
        "        row_emb = self.row_embed(rows)\n",
        "        col_emb = self.col_embed(cols)\n",
        "        pos_encoding = tf.concat([row_emb, col_emb], axis=-1)\n",
        "\n",
        "        x = self.input_projection(x)\n",
        "        x += pos_encoding\n",
        "\n",
        "        x = tf.reshape(x, (-1, 6 * 7, self.d_model))\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x)\n",
        "\n",
        "        column_queries = tf.tile(tf.range(7)[None, :, None], [batch_size, 1, self.d_model])\n",
        "        context = self.column_attention(column_queries, x)\n",
        "\n",
        "        return self.output_layer(context[:, :, 0])\n",
        "\n",
        "\n",
        "###################\n",
        "\n",
        "#@anvil.server.callable\n",
        "def predict_move(board_state):\n",
        "    \"\"\"Receives board state from Anvil, returns AI's move.\"\"\"\n",
        "\n",
        "    # Load the model with custom objects\n",
        "    model = load_model('Converted_8H_10E_64B.keras', custom_objects={'TransformerBlock': TransformerBlock, 'Connect4Transformer': Connect4Transformer})\n",
        "\n",
        "    # Print model summary to verify\n",
        "    # model.summary()\n",
        "\n",
        "    # Convert board to model input shape\n",
        "    input_board = np.array(board_state).reshape(1, 6, 7, 2)\n",
        "    # print(input_board)\n",
        "    # print(input_board.shape)\n",
        "\n",
        "    # Predict best move\n",
        "    predictions = model.predict(input_board)\n",
        "    best_move = int(np.argmax(predictions[0].flatten()))\n",
        "\n",
        "    return best_move\n",
        "\n",
        "\n",
        "board =initialize_board()\n",
        "ai_type = \"CNN\"\n",
        "current_player = 1\n",
        "\n",
        "\n",
        "column = 4\n",
        "board = drop_token(board, column, 1)\n",
        "\n",
        "\n",
        "current_player = 2\n",
        "ai_move = predict_move(board)\n",
        "ai_move"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rKdL5Cdi5oM",
        "outputId": "a9084b36-71ec-4d7c-c63e-87e95beb8cb7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Anvil successfully!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 612ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}