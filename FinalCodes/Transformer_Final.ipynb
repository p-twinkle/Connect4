{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBEhpgBPtKme"
      },
      "outputs": [],
      "source": [
        "## Working Tx code\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, d_model=64, num_heads=4, ff_dim=128, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation='gelu'),\n",
        "            layers.Dense(d_model)\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout)\n",
        "        self.dropout2 = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.attn(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"d_model\": 64,\n",
        "            \"num_heads\": 4,\n",
        "            \"ff_dim\": 128,\n",
        "            \"dropout\": 0.1\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "class Connect4Transformer(Model):\n",
        "    def __init__(self, d_model=64, num_heads=4, ff_dim=128, num_layers=3, dropout=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)  # Pass **kwargs to the parent class\n",
        "        self.d_model = d_model\n",
        "        self.input_projection = layers.Dense(d_model)\n",
        "        self.row_embed = layers.Embedding(input_dim=6, output_dim=d_model // 2)\n",
        "        self.col_embed = layers.Embedding(input_dim=7, output_dim=d_model // 2)\n",
        "        self.transformer_blocks = [TransformerBlock(d_model, num_heads, ff_dim, dropout) for _ in range(num_layers)]\n",
        "        self.column_attention = layers.MultiHeadAttention(num_heads=2, key_dim=d_model)\n",
        "        self.output_layer = layers.Dense(7, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.reshape(inputs, (-1, 6, 7, 2))\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        rows = tf.tile(tf.range(6)[None, :, None], [batch_size, 1, 7])\n",
        "        cols = tf.tile(tf.range(7)[None, None, :], [batch_size, 6, 1])\n",
        "\n",
        "        row_emb = self.row_embed(rows)\n",
        "        col_emb = self.col_embed(cols)\n",
        "        pos_encoding = tf.concat([row_emb, col_emb], axis=-1)\n",
        "\n",
        "        x = self.input_projection(x)\n",
        "        x += pos_encoding\n",
        "\n",
        "        x = tf.reshape(x, (-1, 6 * 7, self.d_model))\n",
        "        for transformer in self.transformer_blocks:\n",
        "            x = transformer(x)\n",
        "\n",
        "        column_queries = tf.tile(tf.range(7)[None, :, None], [batch_size, 1, self.d_model])\n",
        "        context = self.column_attention(column_queries, x)\n",
        "\n",
        "        return self.output_layer(context[:, :, 0])\n",
        "\n",
        "\n",
        "def load_and_preprocess(data):\n",
        "    X = data.iloc[:, :-1].values.astype(np.float32)\n",
        "    y = data.iloc[:, -1].values.astype(np.int32)\n",
        "    X = X.reshape(-1, 6, 7, 2)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "def create_model():\n",
        "    model = Connect4Transformer(\n",
        "        d_model=64,\n",
        "        num_heads=8,\n",
        "        ff_dim=128,\n",
        "        num_layers=3,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(3e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.read_csv(\"converted_board_with_play_y.csv\")\n",
        "    X_train, X_val, y_train, y_val = load_and_preprocess(df)\n",
        "    model = create_model()\n",
        "\n",
        "    # Build the model by calling it once\n",
        "    dummy_input = tf.zeros((1, 6, 7, 2))\n",
        "    _ = model(dummy_input)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        batch_size=64,\n",
        "        epochs=25\n",
        "    )\n",
        "\n",
        "    model.save('txmodel_25E.h5')\n",
        "    model.save('txmodel_25E.keras')"
      ]
    }
  ]
}